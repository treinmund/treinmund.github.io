---
title: "Machine Learning in Practice"
subtitle: "An Annotated Bibliography"
description: |
  This page includes a work-in-progress annotated bibliography of research on how machine learning applications are used in people's work practices. By focusing on situated instances of technology-in-use, these studies provide rich descriptions of people's experiences with and perceptions of machine learning applications to support future designers.
  
  If you know of any studies that are not included in this list, please send them to my email address: tyler[dot]reinmund[at]cs.ox.ac.uk
format:
  html:
    page-layout: full
---

```{r}
#| label: hai-table
#| tbl-cap-location: top
#| echo: false
#| message: false

library(tidyverse)
library(DT)
library(gt)

haibib <- tribble(
  ~Paper, ~Link, ~Authors, ~Year, ~Venue, ~Summary,
  "Evaluating the Promise of Human-Algorithm Collaborations in Everyday Work Practices", "https://doi.org/10.1145/3359245", "Wolf, Christine and Blomberg, Jeanette", 2019, "CSCW", "<p>This paper presents results on the design and evaluation of an NLP tool to support IT architects during the requirements definition stage of solution design projects. Their results focus on three activities supported by the tool, highlighting that the positive expectations held by the architects are underlined by the challenges they currently experience. For example, the architects relish the potential for ML support on a laborious task, but note that the current design of the tool does not facilitate coordination amongst teammates and are concerned that the tool — based on data from historical cases — limits their ability to propose solutions that align with emerging market trends. One of the main takeaways the authors draw from their findings is that ML should not be envisioned as a tool to supplant human decision-making; rather, the capabilities of ML and those of humans should complement one another, accounting for the other’s deficiencies.<p>
  
  <p>This paper provides several solid examples for how the design of ML-based applications do not support the work practices of the IT architects. The authors go at lengths to signal the continuity between their work and past studies on “situated work practices,” as they call it. It leaves me wondering what aspects of ML make the problem of designing for practices any different or more difficult for traditional software systems.<p>",
  
  "To Engage or Not to Engage with AI for Critical Judgments: How Professionals Deal with Opacity When Using AI for Medical Diagnosis", "https://doi.org/10.1287/orsc.2021.1549", "Sarah Lebovitz, Hila Lifshitz-Assaf, Natalia Levina", 2022, "Org. Science", "<p>Over 10 months of ethnographic fieldwork in a large US hospital, Lebovtiz et al. report on the use of three different AI tools by radiologists in lung cancer, breast cancer, and bone age measurement imaging departments. The main finding that the authors put forward is the concept of “AI interrogation practices.” These refer to the extra work radiologists put in to build an understanding of a model’s predictions when they diverge from their own judgment. Importantly, not all radiologists across the three departments employed interrogation practices; the authors suggest that future research should explore why this is the case.<p>
  
  <p>There are a few trajectories left unanswered. The study showed that there were a greater number of occurrences when the radiologists’ judgments differed from the AI tools. How does the skewed ratio of convergences to divergences affect the radiologists’ experiences with the tools? Divergences can also take multiple forms. There may be instances when the tool identifies a nodule, for example, in an image, but the radiologist does not; and vice versa, the tool may not pick up anything anomalous in an image, whereas the radiologist notices an indication of cancerous cells. How did the actions of radiologists vary across these two types of divergence? Lastly, it would be interesting to explore whether the radiologists who performed the “interrogation practices” helped their patients achieve better health outcomes.<p>",
  
  "A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy", "https://doi.org/10.1145/3313831.3376718", "Beede, Emma and Baylor, Elizabeth and Hersch, Fred and Iurchenko, Anna and Wilcox, Lauren and Ruamviboonsuk, Paisan and Vardoulakis, Laura M.", 2020, "CHI", "This paper reports on a contextual evaluation of a deep learning system used for diabetic retinopathy detection in health clinics in Thailand. The authors conduct pre- and post-deployment observations and interviews to understand the nurses workflow with and without the DL system, and evaluate the system’s performance as it is used in a clinical setting. The study points to the importance of contextual evaluations, as they can reveal social and environmental factors which impact model performance that would otherwise be abstracted from in a laboratory setting.",
  
  "“Brilliant AI Doctor” in Rural Clinics: Challenges in AI-Powered Clinical Decision Support System Deployment", "https://doi.org/10.1145/3411764.3445432", "Wang, Dakuo and Wang, Liuping and Zhang, Zhan and Wang, Ding and Zhu, Haiyi and Gao, Yvonne and Fan, Xiangmin and Tian, Feng", 2021, "CHI", "<p>This paper reports on a field study with clinicians in rural China who use a AI clinical decision support system during diagnosis. Their results provide an empirical description of how the use of AI-CDSS in rural clinics is shaped by specific policy, institutional, and medical factors (e.g., there is high demand at rural clinics in China, health care policy limits what services smaller clinics can provide) and outline how clinicians perceive the use of the AI-CDSS.<p>

<p>The paper provides a useful venture into uncharted empirical territory, since most work to date has focused on North America and Europe. Also, their findings resonate with established themes in studies on information systems in health and social care (e.g., workflow integration, interoperability, professional autonomy). The authors offer a fairly light description of their methods, and they do not outline how long they spent at each clinic, whether there were any differences between clinics, and how those related to the observed challenges.<p>",

"“Why Do I Care What’s Similar?” Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts", "https://doi.org/10.1145/3532106.3533556", "Kawakami, Anna and Sivaraman, Venkatesh and Stapleton, Logan and Cheng, Hao-Fei and Perer, Adam and Wu, Zhiwei Steven and Zhu, Haiyi and Holstein, Kenneth", 2022, "DIS", "<p>The authors report on design workshops to improve the interface design of an algorithmic tool used in child protection call screening in the US. They present 10 design concepts which fall broadly into four themes: engaging with discrepancies between human decisions and model predictions, facilitating interactive machine learning, visualising uncertainty, and providing feedback on decision making performance. After outlining the design concepts, the authors elicit feedback on them from 12 social workers in regards to perceived helpfulness, areas for improvement, and envisioned impact on decision making.<p>

<p>Overall, the study leads to two main takeaways. First, the practices of some users may differ fundamentally from the assumptions built into machine learning. In this case, the social workers disagreed with the assumption that current cases can be compared to past, “similar” cases; their perspective holds that everyone is different and what matters when making a decision about someone’s case is their case, not anyone else’s. Second, the authors provide a use case for effective engagement with users in the design of ML-based software. The approach followed seems quite in-depth and engaging, based on observational studies, interviews, and design probes.<p>

<p>This study can be built upon by focusing on instances where the problem is not to re-design an existing interface, but rather to design an entirely new tool based on the needs of the users. It feels that they are trying to improve a “solution” that may not have been the right solution in the first place. Additionally, how could these efforts be extended beyond the interface? Similarly, this focus on the interface exclusively may have precluded them from considering changes that portray call screening as a collaborative practice; there seems to be the assumption that this work is done individually.<p>",

"Shifting Concepts of Value: Designing Algorithmic Decision-Support Systems for Public Services", "https://doi.org/10.1145/3419249.3420149", "Naja Holten Møller, Irina Shklovski, and Thomas T. Hildebrandt", 2020, "NordiCHI", "<p>This paper reports on a series of participatory design workshops held with case workers, data scientists, and system developers that were involved in the development of an algorithmic decision-support system for job placement services in Denmark. The findings point to how the legitimacy of such tools is questioned by the anticipated enduser because of dissonances between how the enduser perceives their role and the role implied by the use of predictive tools, and how the applications of decision-support tools can be re-negotiated.<p>

<p>The methods of the paper are quite interesting, drawing from observational studies, interviews, and PD workshops; it allows them to gather a lot of interesting data from a range of participants. It would have been nice to see what decisions were ultimately settled on, and how power differences affected the discussions in the PD workshops. For example, did data scientists have more organisational power than the case workers? And how about the managers who ostensibly conceived of the decision support tool in the first place?<p>",

"Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making", "https://doi.org/10.1145/3290605.3300234", "Cai, Carrie J. and Reif, Emily and Hegde, Narayan and Hipp, Jason and Kim, Been and Smilkov, Daniel and Wattenberg, Martin and Viegas, Fernanda and Corrado, Greg S. and Stumpe, Martin C. and Terry, Michael", 2019, "CHI", "A group of Google researchers evaluate the performance and acceptance of a content-based image retrieval system with interactive elements for use in pathology diagnoses. After engaging with three pathologists on their perspectives of CBIR systems, the researchers conduct a user study with a cohort of twelve pathologists, and ask them to evaluate the system in comparison to a traditional interface along measures of diagnostic utility, mental support for decision-making, workload, trust, future use, and overall preference.  One of most interesting finding that comes from this paper is that the use of refinement tools (i.e. interactive ML) enables system users to develop mental models of the ML algorithm, thereby increasing their willingness to use it.",

"Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes", "https://doi.org/10.1145/3290605.3300468", "Yang, Qian and Steinfeld, Aaron and Zimmerman, John", 2019, "CHI", "<p>Clinical decision support tools (DSTs) perform well in the lab, but often fail when implemented. Commonly cited reasons for this failure is that the design does not consider clinicians workflows and the collaborative nature of clinicians work. The study designs a clinical DST to support in the VAD implant decision process, and evaluate their design in three US hospitals. Their evaluation seeks to answer the questions: “(1) would clinicians naturally encounter the DST within their current workflow? (2) Would clinicians accept computational decision support in the public context of the meeting? (3) Does placing the prediction in the corner present the right amount of unremarkability?” (p. 4). The authors find that the recurrent multidisciplinary implant decision meeting is the time and place where clinicians are most likely to encounter a DST, and that there is not much resistance to its use in that setting. Physicians, surgeons, and mid-levels each saw a benefit in its use, and these tended to vary across their roles. But, there were a series of challenges offered by the VAD team during the evaluation. Clinicians were considered with the credibility of the model (its data source, whether its been validated in clinical trials, and where its been published), the ethics of using a prediction based on population-level statistics for individual patient decisions, and the static representation of the prediction.<p>

<p>Overall, this is a great paper that includes significant detail on the concerns of physicians, potential incompatibilities between DSTs and their actual decision making processes, and factors researchers should consider when designing their own studies. But, I am left with the question of why did they choose VAD implants as the right place to intervene in. They do not mention that it is a particularly complicated decision, nor is it prone to significant human error. I get the sense that they are trying to squeeze ML into this area just for the sake of it.<p>",
)

haibib <- haibib %>%
  mutate(
    Link = map(Link, ~ htmltools::a(href = .x, "Source")),
    Link = map(Link, ~ gt::html(as.character(.x)))
    )

datatable(haibib,
          extensions = c("Buttons", "Scroller"),
          escape = FALSE,
          options = list(dom = "Bltp",
                         class = "display",
                         buttons = "excel"
                         #scrollY = 200,
                         #scrollX = FALSE,
                         #scroller = TRUE
                         )
          )
```
